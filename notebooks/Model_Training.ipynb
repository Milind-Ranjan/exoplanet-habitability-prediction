{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Exoplanet Habitability Prediction - Model Training\n",
        "\n",
        "This notebook focuses on building and training machine learning models to predict exoplanet habitability based on planetary and stellar characteristics.\n",
        "\n",
        "## Table of Contents\n",
        "1. [Data Preprocessing](#preprocessing)\n",
        "2. [Feature Engineering](#feature-engineering)\n",
        "3. [Habitability Target Creation](#target-creation)\n",
        "4. [Model Training](#model-training)\n",
        "5. [Model Evaluation](#evaluation)\n",
        "6. [Model Selection](#selection)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
        "from sklearn.impute import SimpleImputer\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "import joblib\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and prepare the data\n",
        "df = pd.read_csv('../data/exoplanet.csv', comment='#')\n",
        "\n",
        "# Select key features for habitability prediction\n",
        "features = [\n",
        "    'pl_rade', 'pl_radj', 'pl_bmasse', 'pl_bmassj', 'pl_orbper', \n",
        "    'pl_orbsmax', 'pl_eqt', 'st_teff', 'st_rad', 'st_mass', 'sy_dist'\n",
        "]\n",
        "\n",
        "# Create feature DataFrame\n",
        "X = df[features].copy()\n",
        "\n",
        "print(f\"Feature matrix shape: {X.shape}\")\n",
        "print(f\"Features selected: {features}\")\n",
        "\n",
        "# Display basic statistics\n",
        "display(X.describe())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create habitability target variable based on scientific criteria\n",
        "def create_habitability_target(df):\n",
        "    \"\"\"\n",
        "    Create habitability target based on multiple criteria:\n",
        "    - Planet radius: 0.5 to 2.0 Earth radii (potentially rocky)\n",
        "    - Equilibrium temperature: 200K to 320K (liquid water possible)\n",
        "    - Stellar temperature: 3000K to 7000K (stable main sequence)\n",
        "    - Orbital period: reasonable range for habitable zone\n",
        "    \"\"\"\n",
        "    \n",
        "    conditions = []\n",
        "    \n",
        "    # Planet radius condition (Earth-like size)\n",
        "    if 'pl_rade' in df.columns:\n",
        "        radius_condition = (df['pl_rade'] >= 0.5) & (df['pl_rade'] <= 2.0)\n",
        "        conditions.append(radius_condition)\n",
        "    \n",
        "    # Equilibrium temperature condition (habitable range)\n",
        "    if 'pl_eqt' in df.columns:\n",
        "        temp_condition = (df['pl_eqt'] >= 200) & (df['pl_eqt'] <= 320)\n",
        "        conditions.append(temp_condition)\n",
        "    \n",
        "    # Stellar temperature condition (main sequence stars)\n",
        "    if 'st_teff' in df.columns:\n",
        "        stellar_temp_condition = (df['st_teff'] >= 3000) & (df['st_teff'] <= 7000)\n",
        "        conditions.append(stellar_temp_condition)\n",
        "    \n",
        "    # Combine all conditions\n",
        "    if conditions:\n",
        "        habitability = conditions[0]\n",
        "        for condition in conditions[1:]:\n",
        "            habitability = habitability & condition\n",
        "    else:\n",
        "        habitability = pd.Series([False] * len(df))\n",
        "    \n",
        "    return habitability.astype(int)\n",
        "\n",
        "# Create target variable\n",
        "y = create_habitability_target(df)\n",
        "\n",
        "print(f\"Habitability distribution:\")\n",
        "print(y.value_counts())\n",
        "print(f\"\\nHabitable percentage: {(y.sum() / len(y)) * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data preprocessing and cleaning\n",
        "print(\"DATA PREPROCESSING\")\n",
        "print(\"=\" * 30)\n",
        "\n",
        "# Remove rows where all key features are missing\n",
        "X_clean = X.dropna(subset=['pl_rade', 'pl_eqt', 'st_teff'], how='all')\n",
        "y_clean = y[X_clean.index]\n",
        "\n",
        "print(f\"Original dataset size: {X.shape[0]:,}\")\n",
        "print(f\"After removing rows with missing key features: {X_clean.shape[0]:,}\")\n",
        "\n",
        "# Handle missing values with median imputation\n",
        "imputer = SimpleImputer(strategy='median')\n",
        "X_imputed = pd.DataFrame(\n",
        "    imputer.fit_transform(X_clean),\n",
        "    columns=X_clean.columns,\n",
        "    index=X_clean.index\n",
        ")\n",
        "\n",
        "print(f\"Features after imputation: {X_imputed.shape[1]}\")\n",
        "print(f\"Remaining samples: {X_imputed.shape[0]:,}\")\n",
        "\n",
        "# Check target distribution after cleaning\n",
        "print(f\"\\nTarget distribution after cleaning:\")\n",
        "print(y_clean.value_counts())\n",
        "print(f\"Habitable percentage: {(y_clean.sum() / len(y_clean)) * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train-test split and feature scaling\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_imputed, y_clean, test_size=0.2, random_state=42, stratify=y_clean\n",
        ")\n",
        "\n",
        "print(f\"Training set size: {X_train.shape[0]:,}\")\n",
        "print(f\"Test set size: {X_test.shape[0]:,}\")\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(f\"Training set habitable percentage: {(y_train.sum() / len(y_train)) * 100:.2f}%\")\n",
        "print(f\"Test set habitable percentage: {(y_test.sum() / len(y_test)) * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model training and evaluation\n",
        "print(\"MODEL TRAINING AND EVALUATION\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Initialize models\n",
        "models = {\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "    'XGBoost': xgb.XGBClassifier(random_state=42, eval_metric='logloss'),\n",
        "    'LightGBM': lgb.LGBMClassifier(random_state=42, verbose=-1)\n",
        "}\n",
        "\n",
        "# Train and evaluate models\n",
        "model_scores = {}\n",
        "trained_models = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\nTraining {name}...\")\n",
        "    \n",
        "    # Train model\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    trained_models[name] = model\n",
        "    \n",
        "    # Cross-validation\n",
        "    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='f1')\n",
        "    \n",
        "    # Test predictions\n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
        "    \n",
        "    # Calculate metrics\n",
        "    test_f1 = sklearn.metrics.f1_score(y_test, y_pred)\n",
        "    test_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "    \n",
        "    model_scores[name] = {\n",
        "        'CV_F1_mean': cv_scores.mean(),\n",
        "        'CV_F1_std': cv_scores.std(),\n",
        "        'Test_F1': test_f1,\n",
        "        'Test_AUC': test_auc\n",
        "    }\n",
        "    \n",
        "    print(f\"Cross-validation F1: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
        "    print(f\"Test F1 Score: {test_f1:.4f}\")\n",
        "    print(f\"Test AUC Score: {test_auc:.4f}\")\n",
        "\n",
        "# Display results summary\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MODEL COMPARISON SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "results_df = pd.DataFrame(model_scores).T\n",
        "display(results_df.round(4))\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
